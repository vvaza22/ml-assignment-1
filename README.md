# ML Assignment 1 - House Prices

კონკურსში მოცემული გვაქვს სახლების მონაცემთა ბაზა სხვადასხვა პარამეტრებით და target SalePrice, რომელიც არის სახლის გაყიდვის ფასი. ჩვენი დავალებაა, რომ გავწვრთნათ ისეთი მოდელი, რომელსაც სახლის პარამეტრებს გადავცემთ და შეეცდება გამოიცნოს, რა ფასად გაიყიდება მოცემული სახლი.

Submission-ები ფასდება Root-Mean-Squared-Error(RMSE) ფუნქციით, რაც იმას ნიშნავს, რომ ჩემი მოდელის პატარა ცდომილებებს შედარებით ლმობიერად მოეკიდება, თუმცა რეალური target-დან დიდ აცდენას განსაკუთრებით დასჯის, იქიდან გამომდინარე რომ ცდომილება კვადრატში ადის.


# ჩემი მიდგომა პრობლემის გადასაჭრელად

მონაცემების train და validation set-ად გაჭრის შემდეგ, ვაკვირდები მხოლოდ train set-ს და ვცდილობ მხოლოდ ამ სიმრავლეზე დაყრდნობით მივიღო გადაწყვეტილებები მოდელის ჰიპერპარამეტრების შესახებ.

თავიდანვე ვხვდები, რომ მხოლოდ ის feature-ები უნდა შევარჩიო, რომლებსაც კორელაცია გააჩნიათ SalePrice-თან. რადგანაც ჩვენს მონაცემებს ძალიან ბევრი feature არ აქვს და შესაძლებელია ხელით დავაკვირდე, გადავწყვიტე, რომ ამეგო scatter plot, რომელზეც დავიტანდი SalePrice vs. Feature ორ განზომილებიან სივრცეში, სადაც Feature ერთ-ერთი პარამეტრია train dataset-დან. ამ გზით შემიძლია დავაკვირდე თვალით რაიმე შესამჩნევი კორელაცია არსებობს თუ არა მონიშნულ feature-სა და SalePrice-ს შორის და პირველ რიგში მხოლოდ ის feature-ები შევარჩიო, რომელთათვისაც კორელაცია აშკარაა. მაგალითად ცხადად გამოჩნდა scatter plot-ში, რომ OverallQual წრფივად და პირდაპირპროპორციულად იზრდება ფასთან ერთად, თუმცა მსგავსი კავშირი არ შეიმჩნევა YrSold-სა და MoSold-ზე. აქ ვხდები, რომ ასეთი შერჩევის მეთოდით შეიძლება, რაღაც მნიშვნელოვანი დეტალი გამომრჩეს, რადგან მხოლოდ 2 ცვლადს ვაკვირდები. მაგალითად YrSold-მა შეიძლება სხვა დამატებით პარამეტრთან ერთად მომცეს კორელაცია. შესაძლოა, რაღაცა ტიპის სახლები 2006-2008 წლებში უფრო ძვირად იყიდებოდა და 2008 წლიდან მათი ფასი დაეცა, თუმცა სხვა ტიპის სახლების ფასმა აიწია და საბოლოოდ ისე გამოვიდა, რომ მხოლოდ YrSold-ს არ ეტყობა არანაირი დამოკიდებულება SalePrice-თან, თუმცა YrSold პირობაში სხვა ცვლადს გააჩნია კორელაცია. ამ ყველაფრის მიუხედავად, თავდაპირველი მოდელისთვის არ შევარჩევ YrSold-ს და მსგავს ცვლადებს და მხოლოდ თავდაპირველი მოდელის გაწვრთნის და შეფასების მერე ჩავუმატებ მათ, რათა დავაკვირდე მოდელის performance გაუმჯობესდება თუ არა.

ასევე შემიძლია ავაგო განაწილების გრაფიკი თითოეული feature-ისთვის. თუ ძალიან არადაბალანსებულია feature-ის მნიშვნელობები, მაგალითად Utilities feature-ში მხოლოდ 1 მონაცემს აქვს მნიშვნელობა NoSeWa და დანარჩენს სულ აქვთ მნიშვნელობა AllPub. ამ შემთხვევაში მირჩევნია, რომ ეს feature საერთოდ არ გამოვიყენო training-ში, რადგან შეიძლება მოგვცეს overfit მოდელი იმ 1 მონაცემის გამო.

ვაგებ SalePrice-ის PDF განაწილებასაც, რათა დაახლოებით მივხდე, თუ რა range-ში მხვდება ფასები training set-ში და რამხელა ვარიაციას უნდა ველოდე მოდელის პასუხებში.


# Cleaning მიდგომები

NA-ების შესავსებად გადავწყვიტე, რომ numerical feature-ებისთვის training set-დან დამეთვალა მედიანა და NA-ები შემევსო მედიანით, ხოლო categorical feature-ებისთვის დავითვალე მოდა და მოდით შევავსე NA-ები. შევქმენი NAFiller კლასი pipeline-ისთვის და fit()-ს ვაკეთებ მხოლოდ training set-ზე, რათა შემთხვევით არ მოხვდეს data leakage test set-დან.

drop feature threshold-ად ავიღე 55%, რაც იმას ნიშნავს, რომ feature-ში თუ 55%-ზე მეტი NA არის, მაშინ ამ feature-ს აღარ ვიყენებ. feature-ების დადროფვას NAFiller აკონტროლებს.

ამ დავალებაში მაქსიმალურად ვეცადე, რომ Inference-სა და Experiment-ში ერთი და იგივე კოდი არ გამემეორებინა, ამიტომაც მთლიანი Dataset გადაეცემა pipeline-ს და feature-ებს თვითონ pipeline არჩევს.


# Category encoding მიდგომები

კატეგორიათა უმრავლესობა პირდაპირ ითარგმნებოდა რიცხვებში ordinal encoding-ის საშუალებით, რაც იმას ნიშნავს რომ ბიექციური მიმართება ავაგე მსგავსი კატეგორიებისთვის. მაგალითად Quality-ის მნიშვნელობები თუ იყო ["Po", "Fa", "TA", "Gd", "Ex"], შევუსაბამე 0, 1, 2, 3, 4 და ა. შ.
მსგავსი შესაბამისობა გამოვიყენე ისეთი ცვლადებისთვისაც, რომელთვისასაც scatter plot-დან აშკარად ჩანდა ერთი მნიშვნელობა მეორეზე უარესი იყო.
მაგალითად LandSlope-ის Sev მნიშვნელობის ფასები უფრო დაბალ ფასად იყიდებოდა ვიდრე Mod ან Gtl.

რამდენიმე კატეგორიებისთვის გამოვიყენე One-Hot-Encoding, თუმცა ამ encoding-ის გამოყენების დროს ფრთხილად ვიყავი, რათა მონაცემების განზომილება ძალიან არ გამეზარდა.

# რეპოზიტორის სტრუქტურა



